#!/usr/bin/env python

"""
Test our HAIS implementation on example 1a from http://arxiv.org/abs/1205.1925

We name the latent variable 'z' in place of 'a'

The code is set up to estimate the log marginal of several batches (different `x`) concurrently.

When the latent dimensionality, L, is around 5, there is still some considerable variance in the log marginal estimates.
"""


import tensorflow as tf
import numpy as np
from hais import ais, exact
import matplotlib.pyplot as plt
from matplotlib import lines
import seaborn as sns
import scipy.stats as st
import time
tfd = tf.contrib.distributions


#
# Jupyter magic
#
# %load_ext autoreload
# %autoreload 2


#
# Constants
#
# RNG seed
SEED = 41
#
# HMC AIS
N_ITER = 3000
N_CHAINS = 400
STEPSIZE = .01
#
# Model
BATCH_SIZE = 99
M = 36  # x dimensions
L = 3   # z dimensions
SIGMA_N = .1

#
# Seed RNGs
print('Seeding RNGs')
np.random.seed(SEED)
tf.set_random_seed(SEED)


#
# Sample model
print('Sampling model')
#
# Sample phi
phi = st.norm.rvs(size=(M, L)).astype(dtype=np.float32)
#
# Sample z
z = st.norm.rvs(size=(BATCH_SIZE, L)).astype(dtype=np.float32)
z.shape
#
# Sample x
x_loc = (phi@z.T).T
x_loc.shape
px = st.norm(loc=x_loc, scale=SIGMA_N)
x = px.rvs(size=(BATCH_SIZE, M))
x.shape

#
# True marginal likelihood
lp_exact = exact.culpepper1a_log_marginal(x, phi, SIGMA_N)
print('Calculated exact marginal log likelihood(s): mean={:.1f}; sd={:.1f}'.format(
    np.mean(lp_exact), np.std(lp_exact)))

#
# Model
prior_loc = tf.zeros([BATCH_SIZE, N_CHAINS, L])
z_prior = tfd.MultivariateNormalDiag(loc=prior_loc)
tf_phi = tf.constant(phi)
x_ph = tf.placeholder(tf.float32, [BATCH_SIZE, M], name='x')


def log_likelihood(z):
  "The log pdf of the conditional distribution of x given z."
  #
  assert (BATCH_SIZE, N_CHAINS, L) == z.shape
  assert (M, L) == tf_phi.shape
  assert (BATCH_SIZE, M) == x_ph.shape
  loc = tf.squeeze(
      tf.matmul(
          tf.tile(tf.expand_dims(tf.expand_dims(tf_phi, axis=0), axis=0), [BATCH_SIZE, N_CHAINS, 1, 1]),
          tf.expand_dims(z, axis=-1)),
      axis=-1)
  assert (BATCH_SIZE, N_CHAINS, M) == loc.shape
  x_given_z = tfd.MultivariateNormalDiag(loc=tf.cast(loc, tf.float32), scale_identity_multiplier=SIGMA_N)
  return x_given_z.log_prob(
      tf.tile(tf.expand_dims(x_ph, axis=1), [1, N_CHAINS, 1]), name='log_likelihood')


#
# Annealed importance sampling
print('Constructing AIS computation graph')
starttime = time.time()
model = ais.HAIS(prior=z_prior, log_target=log_likelihood, stepsize=STEPSIZE, adapt_stepsize=True)
#
# Set up an annealing schedule
schedule = ais.get_schedule(N_ITER, rad=4, for_calculating_marginal=True)
#
# Set up the computation graph
logw, z_i, eps, smthd_acceptance_rate = model.ais(schedule)
#
# Calculate the log normalizer (aka log marginal), remember batches are in dimension 0, chains in dimension 1
log_normalizer = model.log_normalizer(logw, samples_axis=1)
endtime = time.time()
print('Constructing graph took {:.1f} seconds'.format(endtime - starttime))
#
# Construct and initialise the session
sess = tf.Session()
# merged = tf.summary.merge_all()
# summary_writer = tf.summary.FileWriter('logs')
sess.run(tf.global_variables_initializer())
#
# Run AIS
print('Running AIS')
starttime = time.time()
log_marginal, logw_ais, z_sampled, eps_final, final_smthd_acceptance_rate = \
    sess.run([log_normalizer, logw, z_i, eps, smthd_acceptance_rate], {x_ph: x})
endtime = time.time()
print('AIS took {:.1f} seconds'.format(endtime - starttime))
print('Estimated marginal log likelihood(s): mean={:.1f}; sd={:.1f}'.format(
    np.mean(log_marginal), np.std(log_marginal)))
print('True      marginal log likelihood(s): mean={:.1f}; sd={:.1f}'.format(
    np.mean(lp_exact), np.std(lp_exact)))
rho = np.corrcoef(log_marginal, lp_exact)[0, 1]
print('Correlation between estimates: {:.3f}'.format(rho))
print('Final step sizes: mean={:.1f}; sd={:.1f}'.format(
    np.mean(eps_final), np.std(eps_final)))
print('Final smoothed acceptance rate: mean={:.1f}; sd={:.1f}'.format(
    np.mean(final_smthd_acceptance_rate), np.std(final_smthd_acceptance_rate)))


#
# Plot the output
print('Plotting marginal log likelihoods')
if model.adapt_stepsize:
  fig, (ax, ax_accept, ax_stepsize) = plt.subplots(3, 1, figsize=(8, 12))
else:
  fig, (ax, ax_accept) = plt.subplots(2, 1, figsize=(8, 12))
ax.scatter(log_marginal, lp_exact)
ax.set_xlabel('AIS')
ax.set_ylabel('true')
ax.set_title('Marginal log likelihoods')
xmin, xmax = ax.get_xbound()
ymin, ymax = ax.get_ybound()
lower = max(xmin, ymin)
upper = min(xmax, ymax)
ax.add_line(lines.Line2D([lower, upper], [lower, upper], linestyle='dashed', color='k', alpha=.3))
#
# Acceptance rate
sns.distplot(final_smthd_acceptance_rate.flatten(), ax=ax_accept)
ax_accept.axvline(x=model.target_acceptance_rate, linestyle='dashed', color='k', alpha=.3)
ax_accept.set_title('average acceptance rates (per batch per chain)')
#
# Step sizes
if model.adapt_stepsize:
  sns.distplot(eps_final.flatten(), ax=ax_stepsize)
  ax_stepsize.axvline(x=model.stepsize, linestyle='dashed', color='k', alpha=.3)
  ax_stepsize.set_title('Step sizes (per batch per chain)')
#
fig.savefig('hais-model1a-gaussian.pdf')
